@misc{radford2016,
      title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}, 
      author={Alec Radford and Luke Metz and Soumith Chintala},
      year={2016},
      eprint={1511.06434},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Volz2018,
  author    = {Vanessa Volz and
               Jacob Schrum and
               Jialin Liu and
               Simon M. Lucas and
               Adam M. Smith and
               Sebastian Risi},
  title     = {Evolving Mario Levels in the Latent Space of a Deep Convolutional
               Generative Adversarial Network},
  journal   = {CoRR},
  volume    = {abs/1805.00728},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.00728},
  archivePrefix = {arXiv},
  eprint    = {1805.00728},
  timestamp = {Thu, 19 Sep 2019 07:52:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-00728.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Paganini2018,
  title = {Accelerating Science with Generative Adversarial Networks: An Application to 3D Particle Showers in Multilayer Calorimeters},
  author = {Paganini, Michela and de Oliveira, Luke and Nachman, Benjamin},
  journal = {Phys. Rev. Lett.},
  volume = {120},
  issue = {4},
  pages = {042003},
  numpages = {6},
  year = {2018},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.120.042003},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.120.042003}
}

@ARTICLE{Lan2020,
AUTHOR={Lan, Lan and You, Lei and Zhang, Zeyang and Fan, Zhiwei and Zhao, Weiling and Zeng, Nianyin and Chen, Yidong and Zhou, Xiaobo},   	 
TITLE={Generative Adversarial Networks and Its Applications in Biomedical Informatics},      	
JOURNAL={Frontiers in Public Health},      	
VOLUME={8},      
PAGES={164},     	
YEAR={2020},      	  
URL={https://www.frontiersin.org/article/10.3389/fpubh.2020.00164},       	
DOI={10.3389/fpubh.2020.00164},      	
ISSN={2296-2565},      
ABSTRACT={The basic Generative Adversarial Networks (GAN) model is composed of the input vector, generator, and discriminator. Among them, the generator and discriminator are implicit function expressions, usually implemented by deep neural networks. GAN can learn the generative model of any data distribution through adversarial methods with excellent performance. It has been widely applied to different areas since it was proposed in 2014. In this review, we introduced the origin, specific working principle, and development history of GAN, various applications of GAN in digital image processing, Cycle-GAN, and its application in medical imaging analysis, as well as the latest applications of GAN in medical informatics and bioinformatics.}
}

@INPROCEEDINGS{Antipoy2017,  
author={G. {Antipov} and M. {Baccouche} and J. {Dugelay}}, 
booktitle={2017 IEEE International Conference on Image Processing (ICIP)},   
title={Face aging with conditional generative adversarial networks},   
year={2017},  volume={},  number={},  pages={2089-2093},  
doi={10.1109/ICIP.2017.8296650}}

@article{Banjo2020,
   abstract = {This paper reviews Generative Adversarial Networks (GANs) in detail by discussing the strength of the GAN when compared to other generative models, how GANs works and some of the notable problems with training, tuning and evaluating GANs. The paper also briefly reviews notable GAN architectures like the Deep Convolutional Generative Adversarial Network (DCGAN), and Wasserstein GAN, with the aim of showing how design specifications in these architectures help solve some of the problems with the basic GAN model. All this is done with a view of discussing the application of GANs in cybersecurity studies. Here, the paper reviews notable cybersecurity studies where the GAN plays a key role in the design of a security system or adversarial system. In general, from the review, one can observe two major approaches these cybersecurity studies follow. In the first approach, the GAN is used to improve generalization to unforeseen adversarial attacks, by generating novel samples that resembles adversarial data which can then serve as training data for other machine learning models. In the second approach, the GAN is trained on data that contains authorized features with the goal of generating realistic adversarial data that can thus fool a security system. These two approaches currently guide the scope of modern cybersecurity studies with generative adversarial networks.},
   author = {Chika Yinka-Banjo and Ogban Asuquo Ugot},
   doi = {10.1007/s10462-019-09717-4},
   issn = {15737462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   keywords = {Cybersecurity,Deep Learning,Generative adversarial networks},
   month = {3},
   pages = {1721-1736},
   publisher = {Springer},
   title = {A review of generative adversarial networks and its application in cybersecurity},
   volume = {53},
   year = {2020},
}
@inproceedings{torrado2018deep,
  title={Deep Reinforcement Learning for General Video Game AI},
  author={Torrado, Ruben Rodriguez and Bontrager, Philip and Togelius, Julian and Liu, Jialin and Perez-Liebana, Diego},
  booktitle={Computational Intelligence and Games (CIG), 2018 IEEE Conference on},
  year={2018},
  organization={IEEE}
}
@misc{schrum2020,
      title={Interactive Evolution and Exploration Within Latent Level-Design Space of Generative Adversarial Networks}, 
      author={Jacob Schrum and Jake Gutierrez and Vanessa Volz and Jialin Liu and Simon Lucas and Sebastian Risi},
      year={2020},
      eprint={2004.00151},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann},
 url={http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2011},
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{loshchilov2018fixing,
  title = {Fixing weight decay regularization in {Adam}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017},
  url = {https://arxiv.org/abs/1711.05101}
}

%Illuminated Mario GAN
@article{Fontaine2020,
   abstract = {Recent developments in machine learning techniques have allowed automatic generation of video game levels that are stylistically similar to human-designed examples. While the output of machine learning models such as generative adversarial networks (GANs) is notoriously hard to control, the recently proposed latent variable evolution (LVE) technique searches the space of GAN parameters to generate outputs that optimize some objective performance metric, such as level playability. However, the question remains on how to automatically generate a diverse range of high-quality solutions based on a prespecified set of desired characteristics. We introduce a new method called latent space illumination (LSI), which uses state-of-the-art quality diversity algorithms designed to optimize in continuous spaces, i.e., MAP-Elites with a directional variation operator and Covariance Matrix Adaptation MAP-Elites, to effectively search the parameter space of theGAN along a set of multiple level mechanics. We show the performance of LSI algorithms in three experiments in SuperMario Bros., a benchmark domain for procedural content generation. Results suggest that LSI generates sets of Mario levels that are reliably mechanically diverse as well as playable.},
   author = {Matthew C. Fontaine and Ruilin Liu and Ahmed Khalifa and Julian Togelius and Amy K. Hoover and Stefanos Nikolaidis},
   month = {7},
   title = {Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network},
   url = {http://arxiv.org/abs/2007.05674},
   year = {2020},
}

%Oringinal GANs
@article{Goodfellow2014,
   abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
   author = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
   month = {6},
   title = {Generative Adversarial Networks},
   url = {http://arxiv.org/abs/1406.2661},
   year = {2014},
}

%'Steering' GANs to desirable outputs
@article{Jahanian2019,
  author    = {Ali Jahanian and
               Lucy Chai and
               Phillip Isola},
  title     = {On the "steerability" of generative adversarial networks},
  journal   = {CoRR},
  volume    = {abs/1907.07171},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.07171},
  archivePrefix = {arXiv},
  eprint    = {1907.07171},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-07171.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Gravina2019,
  author    = {Daniele Gravina and
               Ahmed Khalifa and
               Antonios Liapis and
               Julian Togelius and
               Georgios N. Yannakakis},
  title     = {Procedural Content Generation through Quality Diversity},
  journal   = {CoRR},
  volume    = {abs/1907.04053},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.04053},
  archivePrefix = {arXiv},
  eprint    = {1907.04053},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-04053.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%Video Game Level Corpus
@article{VGLC,
Author = {Adam James Summerville and Sam Snodgrass and Michael Mateas and Santiago Onta~{n}'{o}n Villar},
Title = {The VGLC: The Video Game Level Corpus},
Year = {2016},
Journal = {Proceedings of the 7th Workshop on Procedural Content Generation},
}

%Original MAP-Elites
@article{Mouret2015,
  author    = {Jean{-}Baptiste Mouret and
               Jeff Clune},
  title     = {Illuminating search spaces by mapping elites},
  journal   = {CoRR},
  volume    = {abs/1504.04909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.04909},
  archivePrefix = {arXiv},
  eprint    = {1504.04909},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MouretC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%Graph grammer + GAN = Zelda levels
@misc{gutierrez2020,
      title={Generative Adversarial Network Rooms in Generative Graph Grammar Dungeons for The Legend of Zelda}, 
      author={Jake Gutierrez and Jacob Schrum},
      year={2020},
      eprint={2001.05065},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

%RL Zelda level PCG
@misc{justesen2018,
      title={Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation}, 
      author={Niels Justesen and Ruben Rodriguez Torrado and Philip Bontrager and Ahmed Khalifa and Julian Togelius and Sebastian Risi},
      year={2018},
      eprint={1806.10729},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%Lode runner GAN Illumination
@misc{steckel2021,
      title={Illuminating the Space of Beatable Lode Runner Levels Produced By Various Generative Adversarial Networks}, 
      author={Kirby Steckel and Jacob Schrum},
      year={2021},
      eprint={2101.07868},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%Several Games VAE Illumination
@misc{sarkar2021,
      title={Generating and Blending Game Levels via Quality-Diversity in the Latent Space of a Variational Autoencoder}, 
      author={Anurag Sarkar and Seth Cooper},
      year={2021},
      eprint={2102.12463},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
